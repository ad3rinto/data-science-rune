{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcf8dc4",
   "metadata": {},
   "source": [
    "# Data Science Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912fcc5d",
   "metadata": {},
   "source": [
    "![Data Science Workflow](img/ds-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae751f07",
   "metadata": {},
   "source": [
    "## Step 1: Acquire\n",
    "### Explore Problem\n",
    "#### Data Science: Understanding the Problem *(Lesson 00)*\n",
    "- Get the right question:\n",
    "    - What is the **problem** we try to **solve**?\n",
    "    - This forms the **Data Science problem**\n",
    "    - **Examples**\n",
    "        - Sales figure and call center logs: evaluate a new product\n",
    "        - Sensor data from multiple sensors: detect equipment failure\n",
    "        - Customer data + marketing data: better targeted marketing\n",
    "- **Assess situation**\n",
    "    - Risks, Benefits, Contingencies, Regulations, Resources, Requirement\n",
    "- **Define goal**\n",
    "    - What is the **objective**?\n",
    "    - What is the **success criteria**?\n",
    "- **Conclusion**\n",
    "    - Defining the problem is key to successful Data Science projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd86e8",
   "metadata": {},
   "source": [
    "### Identify Data\n",
    "#### Great Places to Find Data *(Lesson 05)*\n",
    "- [UC Irvine Machine Learning Repository!](https://archive.ics.uci.edu/ml/index.php)\n",
    "- [KD Nuggets](https://www.kdnuggets.com/datasets/index.html) Datasets for Data Mining, Data Science, and Machine Learning\n",
    "    - [KD Nuggets](https://www.kdnuggets.com/datasets/government-local-public.html) Government, State, City, Local and Public\n",
    "    - [KD Nuggets](https://www.kdnuggets.com/datasets/api-hub-marketplace-platform.html) APIs, Hubs, Marketplaces, and Platforms\n",
    "    - [KD Nuggets](https://www.kdnuggets.com/competitions/index.html) Analytics, Data Science, Data Mining Competitions\n",
    "- [data.gov](https://www.data.gov) The home of the U.S. Government’s open data\n",
    "- [data.gov.uk](https://data.gov.uk) Data published by central government\n",
    "- [World Health Organization](https://www.who.int/data/gho) Explore a world of health data\n",
    "- [World Bank](https://data.worldbank.org) source of world data\n",
    "- [Kaggle](https://www.kaggle.com) is an online community of data scientists and machine learning practitioners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203c7d3",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "####  Read CSV files *(Lesson 05)*\n",
    "- Comma-Seperated Values ([Wikipedia](https://en.wikipedia.org/wiki/Comma-separated_values))\n",
    "-  Learn more about Excel processing [in this YouTube lesson on CSV](https://youtu.be/LEyojSOg4EI)\n",
    "- [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html): read a comma-separated values (csv) file into **pandas** DataFrame.\n",
    "```Python\n",
    "import pandas as pd\n",
    "data = pd.read_csv('files/aapl.csv', parse_dates=True, index_col=0)\n",
    "```\n",
    "\n",
    "#### Excel files *(Lesson 05)*\n",
    "- Most videly used [spreadsheet](https://en.wikipedia.org/wiki/Spreadsheet)\n",
    "- Learn more about Excel processing [in this lecture](https://www.learnpythonwithrune.org/csv-groupby-processing-to-excel-with-charts-using-pandas-python/)\n",
    "- [`read_excel()`](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html) Read an Excel file into a pandas DataFrame.\n",
    "```Python\n",
    "data = pd.read_excel('files/aapl.xlsx', index_col='Date')\n",
    "```\n",
    "\n",
    "#### Parquet files *(Lesson 05)*\n",
    "- [Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) is a free open source format\n",
    "- Compressed format\n",
    "- [`read_parquet()`](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html) Load a parquet object from the file path, returning a DataFrame.\n",
    "```Python\n",
    "data = pd.read_parquet('files/aapl.parquet')\n",
    "```\n",
    "\n",
    "#### Web Scraping *(Lesson 03)*\n",
    "- Extracting data from websites\n",
    "- Leagal issues: [wikipedia.org](https://en.wikipedia.org/wiki/Web_scraping#Legal_issues)\n",
    "- [`read_html()`](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html) Read HTML tables into a list of DataFrame objects.\n",
    "```Python\n",
    "url = \"https://en.wikipedia.org/wiki/Wikipedia:Fundraising_statistics\"\n",
    "data = pd.read_html(url)\n",
    "```\n",
    "\n",
    "#### Databases *(Lesson 04)*\n",
    "- [`read_sql()`](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html) Read SQL query or database table into a DataFrame.\n",
    "- The [sqlite3](https://docs.python.org/3/library/sqlite3.html) is an interface for SQLite databases.\n",
    "```Python\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "conn = sqlite3.connect('files/dallas-ois.sqlite')\n",
    "data = pd.read_sql('SELECT * FROM officers', conn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e6c4ba",
   "metadata": {},
   "source": [
    "### Combine Data *(Lesson 06)*\n",
    "- Often we need to combine data from different sources\n",
    "\n",
    "#### pandas DataFrames\n",
    "- pandas DataFrames can combine data ([pandas cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf))\n",
    "- `concat([df1, df2], axis=0)`: [concat](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) Concatenate pandas objects along a particular axis \n",
    "- `df.join(other.set_index('key'), on='key')`: [join](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html) Join columns of another DataFrame.\n",
    "- `df1.merge(df2, how='inner', on='a')` [merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) Merge DataFrame or named Series objects with a database-style join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d477931",
   "metadata": {},
   "source": [
    "## Step 2: Prepare\n",
    "### Explore Data\n",
    "#### Simple Exploration\n",
    "- [`head()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) Return the first n rows.\n",
    "- [`.shape`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html) Return a tuple representing the dimensionality of the DataFrame.\n",
    "- [`.dtypes`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html) Return the dtypes in the DataFrame.\n",
    "- [`info()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) Print a concise summary of a DataFrame.\n",
    "- [`describe()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) Generate descriptive statistics.\n",
    "- [`isna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html).[`any()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html) Returns if any element is missing.\n",
    "\n",
    "#### Groupby, Counts and Statistics *(Lesson 08)*\n",
    "- Count groups to see the significance across results\n",
    "```Python\n",
    "data.groupby('Gender').count()\n",
    "```\n",
    "- Return the mean of the values over the requested axis.\n",
    "```Python\n",
    "data.groupby('Gender').mean()\n",
    "```\n",
    "- Standard Deviation\n",
    "    - **Standard deviation** is a measure of how dispersed (spread) the data is in relation to the mean.\n",
    "    - Low **standard deviation** means data is close to the mean.\n",
    "    - High **standard deviation** means data is spread out.\n",
    "![Standard deviation](img/std-diagram.png)\n",
    "```Python\n",
    "data.groupby('Gender').std()\n",
    "```\n",
    "- Box plots\n",
    "    - Box plots is a great way to visualize descriptive statistics\n",
    "    - Notice that Q1: 25%, Q2: 50%, Q3: 75%\n",
    "\n",
    "![Box plots](img/box-plot.png)\n",
    "\n",
    "- Make a box plot of the DataFrame columns [plot.box()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.box.html)\n",
    "\n",
    "```Python\n",
    "data.boxplot()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df1a27",
   "metadata": {},
   "source": [
    "### Visualize Data *(Lesson 01)*\n",
    "#### Simple Plot\n",
    "```Python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data = pd.read_csv('files/WorldBank-ATM.CO2E.PC_DS2.csv', index_col=0)\n",
    "data['USA'].plot()\n",
    "```\n",
    "- Adding title and labels\n",
    "    - ```title='Tilte'``` adds the title\n",
    "    - ```xlabel='X label'``` adds or changes the X-label\n",
    "    - ```ylabel='X label'``` adds or changes the Y-label\n",
    "```Python\n",
    "data['USA'].plot(title='US CO2 per capita', ylabel='CO2 (metric tons per capita)')\n",
    "```\n",
    "- Adding ranges\n",
    "    - ```xlim=(min, max)``` or ```xlim=min``` Sets the x-axis range\n",
    "    - ```ylim=(min, max)``` or ```ylim=min``` Sets the y-axis range\n",
    "```Python\n",
    "data['USA'].plot(title='US CO2 per capita', ylabel='CO2 (metric tons per capita)', ylim=0)\n",
    "```\n",
    "- Comparing data\n",
    "```Python\n",
    "data[['USA', 'WLD']].plot(ylim=0)\n",
    "```\n",
    "\n",
    "#### Scatter Plot\n",
    "- Good to see any connection\n",
    "```Python\n",
    "data = pd.read_csv('files/sample_corr.csv')\n",
    "data.plot.scatter(x='x', y='y')\n",
    "```\n",
    "\n",
    "#### Histogram\n",
    "- Identifying quality\n",
    "```Python\n",
    "data = pd.read_csv('files/sample_height.csv')\n",
    "data.plot.hist()\n",
    "```\n",
    "- Identifying outliers\n",
    "```Python\n",
    "data = pd.read_csv('files/sample_age.csv')\n",
    "data.plot.hist()\n",
    "```\n",
    "- Setting bins and figsize\n",
    "```Python\n",
    "data = pd.read_csv('files/WorldBank-ATM.CO2E.PC_DS2.csv', index_col=0)\n",
    "data['USA'].plot.hist(figsize=(20,6), bins=10)\n",
    "```\n",
    "\n",
    "#### Bar Plot\n",
    "- Normal plot\n",
    "```Python\n",
    "data = pd.read_csv('files/WorldBank-ATM.CO2E.PC_DS2.csv', index_col=0)\n",
    "data['USA'].plot.bar()\n",
    "```\n",
    "- Range and columns, figsize and label\n",
    "```Python\n",
    "data[['USA', 'DNK']].loc[2000:].plot.bar(figsize=(20,6), ylabel='CO emmission per capita')\n",
    "```\n",
    "\n",
    "#### Pie Chart\n",
    "- Presenting\n",
    "```Python\n",
    "df = pd.Series(data=[3, 5, 7], index=['Data1', 'Data2', 'Data3'])\n",
    "df.plot.pie()\n",
    "```\n",
    "- Value counts in Pie Charts\n",
    "    - ```colors=<list of colors>```\n",
    "    - ```labels=<list of labels>```\n",
    "    - ```title='<title>'```\n",
    "    - ```ylabel='<label>'```\n",
    "    - ```autopct='%1.1f%%'``` sets percentages on chart\n",
    "```Python\n",
    "(data['USA'] < 17.5).value_counts().plot.pie(colors=['r', 'g'], labels=['>= 17.5', '< 17.5'], title='CO2', autopct='%1.1f%%')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2b338",
   "metadata": {},
   "source": [
    "### Cleaning Data *(Lesson 09)*\n",
    "- [`dropna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) Remove missing values.\n",
    "- [`fillna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) Fill NA/NaN values using the specified method.\n",
    "    - Example: Fill missing values with mean.\n",
    "```Python\n",
    "data = data.fillna(data.mean())\n",
    "```\n",
    "- [`drop_duplicates()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) Return DataFrame with duplicate rows removed.\n",
    "- Working with time series\n",
    "    - [`reindex()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html) Conform Series/DataFrame to new index with optional filling logic.\n",
    "    - [`interpolate()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html) Fill NaN values using an interpolation method.\n",
    "- Resources\n",
    "    - pandas user guide: [Working with missing data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db809945",
   "metadata": {},
   "source": [
    "## Step 3: Analyze\n",
    "### Split into Train and Test *(Lesson 10)*\n",
    "- Assign independent features (those predicting) to `X`\n",
    "- Assign classes (labels/dependent features) to `y`\n",
    "- Divide into training and test sets\n",
    "```Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec359718",
   "metadata": {},
   "source": [
    "### Feature Scaling *(Lesson 11)*\n",
    "- **Feature Scaling** transforms values in the similar range for machine learning algorithms to behave optimal.\n",
    "- **Feature Scaling** can be a problems for **Machine Learing** algorithms on multiple features spanning in different magnitudes.\n",
    "- **Feature Scaling** can also make it is easier to compare results\n",
    "#### Feature Scaling Techniques\n",
    "- **Normalization** is a special case of **MinMaxScaler**\n",
    "    - **Normalization**: Converts values between 0-1\n",
    "```Python\n",
    "(values - values.min())/(values.max() - values.min())\n",
    "```\n",
    "    - **MinMaxScaler**: Between any values\n",
    "- **Standardization** (**StandardSclaer** from sklearn)\n",
    "    - Mean: 0, StdDev: 1\n",
    "```Python\n",
    "(values - values.mean())/values.std()\n",
    "```\n",
    "    - Less sensitive to outliers\n",
    "\n",
    "#### Normalization\n",
    "- [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) Transform features by scaling each feature to a given range.\n",
    "- `MinMaxScaler().fit(X_train)` is used to create a scaler.\n",
    "    - Notice: We only do it on training data\n",
    "```Python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "norm = MinMaxScaler().fit(X_train)\n",
    "X_train_norm = norm.transform(X_train)\n",
    "X_test_norm = norm.transform(X_test)\n",
    "```\n",
    "\n",
    "#### Standarization\n",
    "- [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) Standardize features by removing the mean and scaling to unit variance.\n",
    "```Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler().fit(X_train)\n",
    "X_train_stand = scale.transform(X_train)\n",
    "X_test_stand = scale.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906897cf",
   "metadata": {},
   "source": [
    "### Feature Selection *(Lesson 12)*\n",
    "- **Feature selection** is about selecting attributes that have the greatest impact towards the **problem** you are solving.\n",
    "\n",
    "#### Why Feature Selection?\n",
    "- Higher accuracy\n",
    "- Simpler models\n",
    "- Reducing overfitting risk\n",
    "\n",
    "#### Feature Selection Techniques\n",
    "\n",
    "##### Filter methods\n",
    "- Independent of Model\n",
    "- Based on scores of statistical\n",
    "- Easy to understand\n",
    "- Good for early feature removal\n",
    "- Low computational requirements\n",
    "\n",
    "##### Examples\n",
    "- [Chi square](https://en.wikipedia.org/wiki/Chi-squared_test)\n",
    "- [Information gain](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)\n",
    "- [Correlation score](https://en.wikipedia.org/wiki/Correlation_coefficient)\n",
    "- [Correlation Matrix with Heatmap](https://vitalflux.com/correlation-heatmap-with-seaborn-pandas/)\n",
    "\n",
    "##### Wrapper methods\n",
    "- Compare different subsets of features and run the model on them\n",
    "- Basically a search problem\n",
    "\n",
    "##### Examples\n",
    "- [Best-first search](https://en.wikipedia.org/wiki/Best-first_search)\n",
    "- [Random hill-climbing algorithm](https://en.wikipedia.org/wiki/Hill_climbing)\n",
    "- [Forward selection](https://en.wikipedia.org/wiki/Stepwise_regression)\n",
    "- [Backward elimination](https://en.wikipedia.org/wiki/Stepwise_regression)\n",
    "\n",
    "See more on [wikipedia](https://en.wikipedia.org/wiki/Feature_selection#Subset_selection)\n",
    "\n",
    "##### Embedded methods\n",
    "- Find features that contribute most to the accuracy of the model while it is created\n",
    "- Regularization is the most common method - it penalizes higher complexity\n",
    "\n",
    "##### Examples\n",
    "- [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics))\n",
    "- [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization)\n",
    "- [Ridge Regression](https://en.wikipedia.org/wiki/Ridge_regression)\n",
    "\n",
    "#### Remove constant and quasi constant features\n",
    "- [`VarianceThreshold`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) Feature selector that removes all low-variance features.\n",
    "```Python\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold()\n",
    "sel.fit_transform(data)\n",
    "```\n",
    "#### Remove correlated features\n",
    "- The goal is to find and remove correlated features\n",
    "- Calcualte correlation matrix (assign it to `corr_matrix`)\n",
    "- A feature is correlated to any previous features if the following is true\n",
    "    - Notice that we use correlation 0.8\n",
    "```Python\n",
    "corr_features = [feature for feature in corr_matrix.columns if (corr_matrix[feature].iloc[:corr_matrix.columns.get_loc(feature)] > 0.8).any()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f38c25",
   "metadata": {},
   "source": [
    "### Model Selection ([Lesson 13]())\n",
    "- The process of selecting the model among a collection of candidates machine learning models\n",
    "\n",
    "#### Problem type\n",
    "- What kind of problem are you looking into?\n",
    "    - **Classification**: *Predict labels on data with predefined classes*\n",
    "        - Supervised Machine Learning\n",
    "    - **Clustering**: *Identify similarieties between objects and group them in clusters*\n",
    "        - Unsupervised Machine Learning\n",
    "    - **Regression**: *Predict continuous values*\n",
    "        - Supervised Machine Learning\n",
    "- Resource: [Sklearn cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "\n",
    "#### Model Selection Techniques\n",
    "- **Probabilistic Measures**: Scoring by performance and complexity of model.\n",
    "- **Resampling Methods**: Splitting in sub-train and sub-test datasets and scoring by mean values of repeated runs.\n",
    "\n",
    "#### A few models\n",
    "- [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) Ordinary least squares Linear Regression ([Lesson 08]()).\n",
    "```Python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)\n",
    "y_pred = lin.predict(X_test)\n",
    "r2_score(y_test, y_pred)\n",
    "```\n",
    "- [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) C-Support Vector Classification ([Lesson 10]()).\n",
    "```Python\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "```\n",
    "- [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) Classifier implementing the k-nearest neighbors vote ([Lesson 10]()).\n",
    "```Python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "neigh = KNeighborsClassifier()\n",
    "neigh.fit(X_train.fillna(-1), y_train)\n",
    "y_pred = neigh.predict(X_test.fillna(-1))\n",
    "accuracy_score(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27094430",
   "metadata": {},
   "source": [
    "### Analyze Result\n",
    "This is the main **check-point** of your analysis.\n",
    "- Review the **Problem** and **Data Science problem** you started with.\n",
    "    - The analysis should add value to the **Data Science Problem**\n",
    "    - Sometimes our focus drifts - we need to ensure alignment with original **Problem**.\n",
    "    - Go back to the **Exploration** of the **Problem** - does the result add value to the **Data Science Problem** and the initial **Problem** (which formed the **Data Science Problem**)\n",
    "    - *Example:* As Data Scientist we often find the research itself valuable, but a business is often interested in increasing revenue, customer satisfaction, brand value, or similar business metrics.\n",
    "- Did we learn anything?\n",
    "    - Does the **Data-Driven Insights** add value?\n",
    "    - *Example:* Does it add value to have evidence for: Wealthy people buy more expensive cars.\n",
    "        - This might add you value to confirm this hypothesis, but does it add any value for car manufacturer?\n",
    "- Can we make any valuable insights from our analysis?\n",
    "    - Do we need more/better/different data?\n",
    "    - Can we give any Actionable Data Driven Insights?\n",
    "    - It is always easy to want better and more accurate high quality data.\n",
    "- Do we have the right features?\n",
    "    - Do we need eliminate features?\n",
    "    - Is the data cleaning appropriate?\n",
    "    - Is data quality as expected?\n",
    "- Do we need to try different models?\n",
    "    - Data Analysis is an iterative process\n",
    "    - Simpler models are more powerful\n",
    "- Can result be inconclusive?\n",
    "    - Can we still give recommendations?\n",
    "\n",
    "#### Quote\n",
    "> *“It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts.”* \n",
    "> - Sherlock Holmes\n",
    " \n",
    "#### Iterative Research Process\n",
    "- **Observation/Question**: Starting point (could be iterative)\n",
    "- **Hypothesis/Claim/Assumption**: Something we believe could be true\n",
    "- **Test/Data collection**: We need to gether relevant data\n",
    "- **Analyze/Evidence**: Based on data collection did we get evidence?\n",
    "    - Can our model predict? (a model is first useful when it can predict)\n",
    "- **Conclude**: *Warning!* E.g.: We can conclude a correlation (this does not mean A causes B)\n",
    "    - Example: Based on the collected data we can see a correlation between A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c8d55",
   "metadata": {},
   "source": [
    "## Step 4: Report\n",
    "### Present Findings\n",
    "- You need to *sell* or *tell* a story with the findings.\n",
    "- Who is your **audience**?\n",
    "    - Focus on technical level and interest of your audience\n",
    "    - Speak their language\n",
    "    - Story should make sense to audience\n",
    "    - Examples\n",
    "        - **Team manager**: Might be technical, but often busy and only interested in high-level status and key findings.\n",
    "        - **Data engineer/science team**: Technical exploration and similar interest as you\n",
    "        - **Business stakeholders**: This might be end-customers or collaboration in other business units.\n",
    "- When presenting\n",
    "    - **Goal**: Communicate actionable insights to key stakeholders\n",
    "    - Outline (inspiration):\n",
    "        - **TL;DR** (Too-long; Didn’t read) - clear and concise summary of the content (often one line) that frames key insights in the context of impact on key business metrics.\n",
    "        - Start with your understanding of the business problem\n",
    "        - How does it transform into a Data Science Problem\n",
    "        - How will to measure impact - what business metrics are indicators of results\n",
    "        - What data is available and used\n",
    "        - Presenting hypthosis of reseach\n",
    "        - A visual presentation of the insights (model/analysis/key findings)\n",
    "            - This is where you present the evidence for the insights\n",
    "        - How to use insight and create actions\n",
    "        - Followup and continuous learning increasing value\n",
    "\n",
    "### Visualize Results\n",
    "- Telling a story with the data\n",
    "- This is where you convince that the findings/insights are correct\n",
    "- The right visualization is important\n",
    "    - Example: A correlation matrix might give a Data Engineer insights in how findings where discovered, but confuse business partners.\n",
    "\n",
    "#### Resources for visualization\n",
    "- [Seaborn](https://seaborn.pydata.org) Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "- [Plotly](https://plotly.com) open-source for analytic apps in Python\n",
    "- [Folium](http://python-visualization.github.io/folium/) makes it easy to visualize data that’s been manipulated in Python on an interactive leaflet map.\n",
    "\n",
    "### Credibility Counts\n",
    "- This is the check point if your research is valid\n",
    "    - Are you hiding findings you did not like (not supporting your hypothesis)?\n",
    "    - Remember it is the long-term relationship that counts\n",
    "- Don't leave out results\n",
    "    - We learn from data and find hidden patterns, to make data-driven decisions, with a long-term perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e74bf02",
   "metadata": {},
   "source": [
    "## Step 5: Actions\n",
    "### Use Insights\n",
    "- How do we follow up on the presented **Insights**?\n",
    "- **No one-size-fits-all**: It depends on the **Insights** and **Problem**\n",
    "- *Examples:*\n",
    "    1. **Problem**: What customers are most likely to cancel subscription?\n",
    "        - Say, we have insufficient knowledge of customers, and need to get more, hence we have given recommendations to gather more insights\n",
    "        - But you should still try to add value\n",
    "    2. **Problem**: Here is our data - find valuable insights!\n",
    "        - This is a challenge as there is no given focus\n",
    "        - An iterative process involving the customer can leave you with no surprises\n",
    "\n",
    "### Measure Impact\n",
    "- If customer cannot measure impact of your work - they do not know what they pay for.\n",
    "    - If you cannot measure it - you cannot know if hypothesis are correct.\n",
    "    - A model is first valuable when it can be used to predict with some certainty\n",
    "- There should be identified metrics/indicators to evaluate in the report\n",
    "- This can evolve - we learn along the way - or we could be wrong.\n",
    "- How long before we expect to see impact on identified business metrics?\n",
    "- What if we do not see expected impact?\n",
    "- Understanding of metrics\n",
    "    - The metrics we measure are indicators that our hypthesis is correct\n",
    "    - Other aspects can have impact on the result - but you need to identify that\n",
    "    \n",
    "### Main Goal\n",
    "- Your success of a Data Scientist is to create valuable actionable insights\n",
    "\n",
    "#### A great way to think\n",
    "- Any business/organisation can be thought of as a complex system\n",
    "    - Nobody understands it perfectly and it evolves organically\n",
    "- Data describes some aspect of it\n",
    "- It can be thought of as a black-box\n",
    "- Any insights you can bring is like a window that sheds light on what happens inside"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec293c",
   "metadata": {},
   "source": [
    "## General Advice\n",
    "- **Expectations**\n",
    "    - When I started my PhD (researcher) journey I expected to solve big problems - change the world to a better place\n",
    "    - Reality was different - small incremental contributions\n",
    "    - Start with simple interesting problems - do not expect to find insights that will change the world from day one.\n",
    "- **Learning**\n",
    "    - This is a new field - but like any research field, it evolves and we learn new techniques and get new tools\n",
    "    - This course gave a you a solid basis, but there is a lot more to learn\n",
    "    - Don't expect your learning to end\n",
    "- **Long-term focus**\n",
    "    - Be clear on your goal: Become a Data Scientist\n",
    "    - This will help you when things seems difficult - everyone has times of struggle\n",
    "    - Don't get discouraged by seeing someone else present some awesome work - learn from it\n",
    "- **Curiosity**\n",
    "    - I always say **keep it playful**\n",
    "    - You need to enjoy what you do\n",
    "    - Most people are curious - so let your curiosity guide you on your Data Science journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36a48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
